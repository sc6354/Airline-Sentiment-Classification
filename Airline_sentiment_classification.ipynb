{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600698614752",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# US Airline Twitter Sentiment Analysis using Logistic Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### In this notebook, we will implement a simple logistic regression model to classify a tweet as negative, positive, or neutral. \n",
    "\n",
    "A sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as \"late flight\" or \"rude service\").\n",
    "\n",
    "The dataset is publicly available on [Kaggle] (https://www.kaggle.com/crowdflower/twitter-airline-sentiment) and [Data.world](https://data.world/crowdflower/airline-twitter-sentiment).\n",
    "\n",
    "The goal is not to achieve the highest accuracy but to demonstrate a logistic regression model.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import csv \n",
    "%matplotlib inline"
   ]
  },
  {
   "source": [
    "## Step 1: Load the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = (\"/Users/susanchen/Desktop/Airline_sentiment/Airline-Sentiment-2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom function that takes in a dataframe and extracts the tweets and labels\n",
    "def preprocess_data(datafile):\n",
    "    df = pd.read_csv(datafile)\n",
    "    tweets = np.array(df.text)\n",
    "    labels = np.array(df.airline_sentiment)\n",
    "    return tweets, labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets, labels = preprocess_data(datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array(['@VirginAmerica What @dhepburn said.',\n       \"@VirginAmerica plus you've added commercials to the experience... tacky.\",\n       \"@VirginAmerica I didn't today... Must mean I need to take another trip!\",\n       ...,\n       '@AmericanAir Please bring American Airlines to #BlackBerry10',\n       \"@AmericanAir you have my money, you change my flight, and don't answer your phones! Any other suggestions so I can make my commitment??\",\n       '@AmericanAir we have 8 ppl so we need 2 know how many seats are on the next flight. Plz put us on standby for 4 people on the next flight?'],\n      dtype=object)"
     },
     "metadata": {},
     "execution_count": 217
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "source": [
    "## Step 2: Clean up the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### We start by removing links, symbols, and @mentions from each tweet"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Removes @mentions, links and symbols expect hashtags\n",
    "def cleanTweet(tweetArray):\n",
    "    \n",
    "    punctuation = '!\"#$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~'\n",
    "    all_tweets = 'separator'.join(tweetArray)\n",
    "    all_tweets = all_tweets.lower()\n",
    "    all_text =''.join([t for t in all_tweets if t not in punctuation])\n",
    "\n",
    "    tweets_split = all_text.split('separator')\n",
    "    all_text = ' '.join(tweets_split)\n",
    "\n",
    "    clean_tweets = []\n",
    "    for t in tweets_split:\n",
    "        # remove @mentions\n",
    "        clean_tweet = re.sub(r'@[A-Za-z0-9_]+', '', t)\n",
    "        # Remove any links\n",
    "        clean_tweet = re.sub('https?://[A-Za-z0-9./]+','', t)\n",
    "        # Remove @Airline mentions \n",
    "        clean_tweet = re.sub(\"@[\\w]*\", '',  t)\n",
    "        clean_tweets.append(clean_tweet)\n",
    "    \n",
    "    return clean_tweets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweets = cleanTweet(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[' what  said',\n ' plus youve added commercials to the experience tacky',\n ' i didnt today must mean i need to take another trip',\n ' its really aggressive to blast obnoxious entertainment in your guests faces amp they have little recourse',\n ' and its a really big bad thing about it',\n ' seriously would pay 30 a flight for seats that didnt have this playing\\nits really the only bad thing about flying va',\n ' yes nearly every time i fly vx this ‰ыпear worm‰ыќ won‰ыєt go away ',\n ' really missed a prime opportunity for men without hats parody there httpstcomwpg7grezp',\n ' well i didnt‰ыbut now i do d',\n ' it was amazing and arrived an hour early youre too good to me']"
     },
     "metadata": {},
     "execution_count": 218
    }
   ],
   "source": [
    "# look at the first ten clean tweets \n",
    "clean_tweets[:10]"
   ]
  },
  {
   "source": [
    "#### Stopwords (such as 'the', 'a', 'an', 'in', 'for') carry no sentimental meaning and can up valuable processing time and space in our database, so it is best pratice to remove them. \n",
    "\n",
    "#### NLTK (Natural Language toolkit) in python has a list of stopwords stored in 16 languages. We will only need the english list as our tweets are in English. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/susanchen/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
    }
   ],
   "source": [
    "import nltk\n",
    "# to download stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(tweetArray):\n",
    "    tokens = [t for t in tweetArray if not t in stopwords.words('english')]\n",
    "    tokens = np.array(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = removeStopWords(clean_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([' what  said',\n       ' plus youve added commercials to the experience tacky',\n       ' i didnt today must mean i need to take another trip', ...,\n       ' please bring american airlines to blackberry10',\n       ' you have my money you change my flight and dont answer your phones any other suggestions so i can make my commitment',\n       ' we have 8 ppl so we need 2 know how many seats are on the next flight plz put us on standby for 4 people on the next flight'],\n      dtype='<U168')"
     },
     "metadata": {},
     "execution_count": 384
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "source": [
    "#### Encode the labels as -1 (negative), 0 (neutral), 1(postive)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1=positive, 0=neutral, -1=negative label conversion\n",
    "encoded_labels = []\n",
    "for label in labels:\n",
    "    if label == 'neutral':\n",
    "        encoded_labels.append(0)\n",
    "    elif label == 'negative':\n",
    "        encoded_labels.append(-1)\n",
    "    else:\n",
    "        encoded_labels.append(1)\n",
    "\n",
    "encoded_labels = np.asarray(encoded_labels)\n"
   ]
  },
  {
   "source": [
    "#### Encode tweeets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def vect(token):\n",
    "    vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    lowercase = True)\n",
    "    features = vectorizer.fit_transform(token).toarray()\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vect(tokens)"
   ]
  },
  {
   "source": [
    "## Step 3: Training a Logistic Regression Model on the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Part 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(features, encoded_labels, train_size=0.80, test_size =.20, random_state=1)"
   ]
  },
  {
   "source": [
    "#### Use GridSearchCV to find the optimal parameters "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Best cross-validation score: 0.80\nBest parameters:  {'C': 0.5}\nBest estimator:  LogisticRegression(C=0.5)\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, .5, 1, 10]}\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring= 'accuracy')\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)\n",
    "print(\"Best estimator: \", grid.best_estimator_)"
   ]
  },
  {
   "source": [
    "#### Using 5 rounds of experiments, we find that the optimal c is .5 and the best cross-validation accuracy is 80%. We could stop here, but for demonstrative purposes, I will also show how to fit the model without Grid Search. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Part 2 \n",
    "#### Start by initiating a Logistics Regression Model and then training it. Here we set C equal to the optimal .5 (a smaller value means stronger regularization strength)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = LogisticRegression(C=.5, random_state =1, multi_class = 'multinomial', solver = 'lbfgs')\n",
    "model = log_model.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.7971311475409836\n"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "source": [
    "#### Our accuracy score is .7971 which is about .8 or 80%. This confirms the results from Grid Search. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Part 3 Using Pipelines and Cross Validation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Pipeline(steps=[('model',\n                 LogisticRegression(C=0.5, multi_class='multinomial',\n                                    random_state=1))])"
     },
     "metadata": {},
     "execution_count": 313
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "my_pipeline = Pipeline(steps=[(\"model\", log_model)])\n",
    "my_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "log_model2 = LogisticRegression(C=.5, random_state =1, multi_class = 'multinomial', solver = 'lbfgs')\n",
    "#model2 = log_model2.fit(X=X_train, y=y_train)\n",
    "#y_pred2 = model2.predict(X_test)\n",
    "\n",
    "pipe = Pipeline(steps=[(\"model\", log_model2)])\n",
    "#pipe.fit(X_train, y_train)\n",
    "#pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores2 = cross_val_score(pipe, X= features, y = labels, cv=5, scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy scores: \n \n [0.78995902 0.78722678 0.79849727 0.81352459 0.79678962]\nModel 2 Average accuracy (across experiments):\n0.7972\n"
    }
   ],
   "source": [
    "print('Accuracy scores: \\n \\n', scores2)\n",
    "print(\"Model 2 Average accuracy (across experiments):\")\n",
    "print(scores2.mean().round(4))"
   ]
  },
  {
   "source": [
    "#### Again the accuracy scores were all very close to .8."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Step 4: Make predictions on random tweets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Randomly pick tweets from the test set and predict it's label"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Predition Label: -1\n@USAirways What is with your lost &amp; found.  My wife lost her phone in Philly, &amp; followed your web instructions to call an 800#.  No answer.\n\n\nPredition Label: 1\n@united thx for update\n\n\nPredition Label: 0\n@JetBlue #489. Flight #589 is departing before we even board\n\n\nPredition Label: -1\n@AmericanAir your a LIAR, no precipitation all day, 47 degrees, don't pacify, you look like a fool.......thats the problem, admit you suck\n\n\nPredition Label: 1\n@AmericanAir thanks... I finally got through this afternoon.  :)\n\n\n"
    }
   ],
   "source": [
    "import random\n",
    "j = random.randint(0,len(X_test)-5)\n",
    "for i in range(j,j+5):\n",
    "    print('Predition Label:','{:d}'.format(y_pred[i]))\n",
    "    ind = features.tolist().index(X_test[i].tolist())\n",
    "    print(tweets[ind].strip())\n",
    "    print('\\n')"
   ]
  },
  {
   "source": [
    "#### As you can see, the model was able to accurately predict the tweets with some degree of error. The third tweet: \"@JetBlue #489. Flight #589 is departing before we even board\" was labeled as neutral and could be depending on the context. But as most would say, that tweet express more negativity than neutral feelings. \n",
    " "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Test the Model on made-up tweets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(features, seq_length):\n",
    "    ''' Return features of test tweet, where each tweet feature is padded with 0's \n",
    "        or truncated to the input seq_length. \n",
    "    '''\n",
    "    # getting the correct rows x cols shape\n",
    "    features2 = np.zeros((len(features), seq_length), dtype=int)\n",
    "\n",
    "    # for each test feature, fill with the correct values\n",
    "    for i, row in enumerate(features):\n",
    "        features2[i, -len(row):] = np.array(row)[:seq_length]\n",
    "    \n",
    "    return features2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "prediction: [1 1 0]\n"
    }
   ],
   "source": [
    "test_tweets =[]\n",
    "pos = 'this flight was on time and I just love flying with #Delta :)'\n",
    "neg = '@AmericanAirlines do not fly with American Airlines they are the worst airline and their flights are always late'\n",
    "neut= \"time to board in 30 minutes\"\n",
    "test_tweets.append(pos)\n",
    "test_tweets.append(neg)\n",
    "test_tweets.append(neut)\n",
    "\n",
    "test_clean = cleanTweet(test_tweets)\n",
    "test_token = removeStopWords(test_clean)\n",
    "test_features = pad_features(vect(test_token), 16107)\n",
    "\n",
    "\n",
    "print(\"prediction: {}\". format(model.predict(test_features)))\n"
   ]
  },
  {
   "source": [
    "#### Running the prediction on made-up tweets, we can see that the model did not label the negative tweet as negative, but rather as positive. One potential reason for this could be the portion of positive tweets to negative/neutral tweets. If the model is fitted on a dataset with mostly positive tweets, then predicting a negative tweet may require a different model or more training. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}